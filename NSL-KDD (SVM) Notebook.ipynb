{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning project - analyzing the NSL-KDD data with SVM\n",
    "\n",
    "The KDD99 intrusion detection dataset[1] is one of the most frequently studied datasets of its kind.  This network traffic dataset was originally published in 1999, but other researchers[2] published a revised version in 2009 that was an improved version of the original.  (Most importantly, a very significant number of duplicate records have been removed.)\n",
    "\n",
    "My notebook works with the revised NSL-KDD dataset resulting from the 2009 work.  Since the original NSL-KDD set no longer appears to be hosted online, I've made a copy on my github[3].\n",
    "\n",
    "First, I will do some exploration of the data set, pre-process it to get it into a usable form for SVM modeling. I will explore the hyperparmeter space with a subset of the data.  Once I've selected good values for the SVM hyperparameters, I'll rebuild the model using the entire dataset, and review the performance against the provided test set.\n",
    "\n",
    "[1] http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
    "[2] M. Tavallaee, E. Bagheri, W. Lu, and A. Ghorbani, “A Detailed Analysis of the KDD CUP 99 Data Set” (2009)\n",
    "[3] https://github.com/mwinton/NSL-KDD-Data-Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start by importing the usual required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pylab as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTION 1: Load full data set.\n",
    "\n",
    "If you are planning to skip the hyperparameter optimization below, you can load the full training set now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the full training data if you don't plan to wait for the hyperparameter optimization.\n",
    "raw_data=pd.read_csv('https://raw.githubusercontent.com/mwinton/NSL-KDD-Data-Set/master/KDDTrain+.csv', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTION 2: Load smaller (20% data set).\n",
    "\n",
    "If you are planning to run the hyperparameter optimization (128 permutations explored), then use the following data set.  (Even with this smaller dataset, that optimization step will take ~1 hour.)\n",
    "\n",
    "Or if you just want the final step (building the model and running it against the test data) to run more quickly, you can use this set.  The resulting accuracy is not significantly lower than when using the full training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load a smaller subset of the data for exploration and hyperparameter search\n",
    "raw_data=pd.read_csv('https://raw.githubusercontent.com/mwinton/NSL-KDD-Data-Set/master/20%20Percent%20Training%20Set.csv', header=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test set.\n",
    "\n",
    "No matter which option you chose above to train the data, you will need the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load full test set\n",
    "raw_test_data = pd.read_csv('https://raw.githubusercontent.com/mwinton/NSL-KDD-Data-Set/master/KDDTest+.csv', header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train & test data into a single DataFrame for pre-processing.\n",
    "# Keep track of number of rows of train vs. test in order to re-split them later.\n",
    "num_rows_train = len(raw_data)\n",
    "X_combined = pd.concat([raw_data,raw_test_data], axis=0)\n",
    "\n",
    "# Read feature names from a file and add to the DataFrame\n",
    "fnames = pd.read_csv('https://raw.githubusercontent.com/mwinton/NSL-KDD-Data-Set/master/Field%20Names.csv', header=None)\n",
    "col_names = fnames[0].values\n",
    "col_names = np.append(col_names,['labels','difficulty_level'])\n",
    "X_combined.columns = col_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The attack type table from the dataset was incomplete.  It only lists attacks in the training set, and not those in the test set.  I have manually created a dict with the full list.\n",
    "\n",
    "atype_dict = {'back':'dos','buffer_overflow':'u2r','ftp_write':'r2l',\n",
    "              'guess_passwd':'r2l','imap':'r2l','ipsweep':'probe','land':'dos',\n",
    "              'loadmodule':'u2r','multihop':'r2l','neptune':'dos',\n",
    "              'nmap':'probe','perl':'u2r','phf':'r2l','pod':'dos',\n",
    "              'portsweep':'probe','rootkit':'u2r','satan':'probe','smurf':'dos',\n",
    "              'spy':'r2l','teardrop':'dos','warezclient':'r2l','warezmaster':'r2l',\n",
    "              'normal':'normal','unknown':'unknown',\n",
    "              'apache2':'dos','udpstorm':'dos','processtable':'dos','mailbomb':'dos',\n",
    "              'saint':'probe','mscan':'probe','xterm':'u2r','ps':'u2r',\n",
    "              'sqlattack':'u2r','snmpgetattack':'r2l','named':'r2l','xlock':'r2l',\n",
    "              'xsnoop':'r2l','sendmail':'r2l','httptunnel':'r2l','worm':'r2l',\n",
    "              'snmpguess':'r2l'}\n",
    "\n",
    "# Add a column for attack_type, and delete the originals. We don't need them.\n",
    "X_combined['attack_type'] = X_combined['labels'].map(atype_dict)\n",
    "del X_combined['labels']\n",
    "del X_combined['difficulty_level']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "First, we need to split off the y columns (labels) from the X_combined DataFrame.  We also need to convert them to numeric form for the sklearn SVM algorithms.  We will use LabelEncoder for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert y to numeric labels\n",
    "y_combined=X_combined.pop('attack_type')\n",
    "le = LabelEncoder()\n",
    "le.fit(y_combined)\n",
    "y_combined = le.transform(y_combined)\n",
    "y_labels = le.inverse_transform([0,1,2,3,4]) #generate this for label graphs/reports later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform one-hot encoding on the protocol_type, service, and flag features\n",
    "\n",
    "This is an important step.  Because these are nominal, rather than ordinal values, we cannot simply convert them to integers.  For example, it's not meaningful for an algorithm to say 'tcp' > 'icmp' or vice versa. Instead, we use the one-hot encoder to create a set of columns for each value of the original feature.  These new columns include only 0's and 1's.  (Note: many papers in published literature appear to have dealt with these columns incorrectly.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform one-hot encoding on nominal features\n",
    "ohe_protocol_cols = pd.get_dummies(X_combined['protocol_type'])\n",
    "ohe_service_cols = pd.get_dummies(X_combined['service'])  # 66 columns\n",
    "ohe_flag_cols = pd.get_dummies(X_combined['flag'])\n",
    "X_combined = pd.concat([X_combined,ohe_protocol_cols,ohe_service_cols,ohe_flag_cols],axis=1)\n",
    "\n",
    "drop_cols = ['protocol_type','service','flag']   \n",
    "X_combined  = X_combined.drop(X_combined[drop_cols],axis=1)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split out train vs. test, and then also create a cross-validation set\n",
    "\n",
    "We cannot optimize our hyperparameters by comparing against either the data the model is trained on, or the final test data.  So instead, we will pull out 30% of records from the training data as a cross-validation set.  Hyperparameter optimization will be done on this 30% set.  Once optimal values of the hyperparameters are found, we will re-train the model on the entire training set (including this cross-validation subset). \n",
    "\n",
    "(Note that the random_state flag is set in order to make the experiment repeatable.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Re-split train vs test data\n",
    "X_train = X_combined[:num_rows_train]\n",
    "X_test = X_combined[num_rows_train:]\n",
    "y_train = y_combined[:num_rows_train]\n",
    "y_test = y_combined[num_rows_train:]\n",
    "\n",
    "# Test-train-split for cross-validation\n",
    "X_train,X_cv,y_train,y_cv = train_test_split(X_train, y_train, \n",
    "                                                 random_state = 1, test_size = 0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply feature scaling\n",
    "\n",
    "SVM does not perform well if features are not on similar scales, so feature scaling is a requirement.  Recall that the one-hot encoded columns are already [0,1], so I've chosen to use the MinMaxScaler because it also will output features values in this range.  (Since this seemed the work sufficiently well, I didn't try the alternate StandardScaler.)\n",
    "\n",
    "(Note that I also tried using SelectKBest to reduce the feature space to the top 20 features, but the model didn't perform as well as when I used all features. I have not included that code here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Apply feature scaling to standardize the train/test sets. \n",
    "\n",
    "mmsc = MinMaxScaler()\n",
    "mmsc.fit(X_train)\n",
    "X_train_norm = mmsc.transform(X_train)\n",
    "X_test_norm = mmsc.transform(X_test)\n",
    "X_cv_norm = mmsc.transform(X_cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONAL: optimizing the hyperparameters - THIS TAKES A LONG TIME!\n",
    "\n",
    "This was an important step in the experiment, but it takes a long time (~ 1 hour on my computer).  If you don't want to go through that pain, **DO NOT CLICK** to run the next box.  Instead move on to the next box.\n",
    "\n",
    "I explored both rbf and sigmoid kernels.  I also explored C and sigma values 2^k with k in [-8,-6,-4,-2,0,2,4,6]. This results in 128 permutations that will be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DO NOT RUN THE CODE IN THIS BOX UNLESS YOU ARE PREPARED TO WAIT FOR A VERY LONG TIME!\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "scores = []\n",
    "for k in svm_kernels:\n",
    "    for c in svm_C_exp_vals:\n",
    "        for g in svm_gamma_exp_vals:\n",
    "            \n",
    "            svc = SVC(C = (2**c), kernel = k, gamma = (2**g))\n",
    "            svc.fit(X_train_norm,y_train)\n",
    "            score = svc.score(X_cv_norm,y_cv)\n",
    "            scores.append((k,c,g,score)) # in hindsight, probably better saved as a dict\n",
    "            print('\\nSVM (k=%s C=2**%d sigma=2**%d: accuracy score = %.3f' % (k,c,g,score))\n",
    "\n",
    "\n",
    "# Prepare hyperparameter data for 3d plotting\n",
    "plt_scores = np.array(scores)\n",
    "rbf_scores = plt_scores[plt_scores[:,0]=='rbf'][:,[1,2,3]]\n",
    "sigmoid_scores = plt_scores[plt_scores[:,0]=='sigmoid'][:,[1,2,3]]\n",
    "plot_sets = {'rbf':rbf_scores,'sigmoid':sigmoid_scores}\n",
    "\n",
    "# Plot scatter plot\n",
    "for p in plot_sets:\n",
    "    plt_data = plot_sets[p]\n",
    "    c = plt_data[:,0]\n",
    "    c = c.astype(np.int)\n",
    "    g = plt_data[:,1]\n",
    "    g = s.astype(np.int)\n",
    "    score = plt_data[:,2]\n",
    "    score = score.astype(np.float)\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle('SVM hyperparam. optimization: '+str(p))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(c,g,score)\n",
    "    ax.set_xlabel('log2 C')\n",
    "    ax.set_ylabel('log2 gamma')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Continue on, using optimal hyperparameters from previous grid search.\n",
    "\n",
    "We will regenerate the full training set (including the cross-validation data) and move forward using the following choices: kernel = rbf ; C = 2^6 ; gamma = 1.  These resulted in 99.5% accuracy on the cross-validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the parameters to use going forward\n",
    "k_selected = 'rbf'\n",
    "C_selected = 2**6\n",
    "gamma_selected = 2**0\n",
    "\n",
    "# Regenerate full training set (including CV data)\n",
    "X_train = X_combined[:num_rows_train]\n",
    "X_test = X_combined[num_rows_train:]\n",
    "y_train = y_combined[:num_rows_train]\n",
    "y_test = y_combined[num_rows_train:]\n",
    "\n",
    "# re-run the feature scaling\n",
    "mmsc = MinMaxScaler()\n",
    "mmsc.fit(X_train)\n",
    "X_train_norm = mmsc.transform(X_train)\n",
    "X_test_norm = mmsc.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, we can make predictions for our test set to see how our model performed.\n",
    "\n",
    "Note that the following block of code could take up to ~10 minutes to run if you're using the full training set.  If you're using the smaller ~20% training set, it should run in ~2 minutes.\n",
    "\n",
    "You should see a classification report similar to this one:\n",
    "\n",
    "SVM (k=rbf C=2^64 sigma=2^1: accuracy score = 0.765\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "        dos       0.98      0.79      0.88      7458\n",
    "     normal       0.67      0.97      0.80      9710\n",
    "      probe       0.78      0.64      0.71      2421\n",
    "        r2l       0.63      0.11      0.18      2887\n",
    "        u2r       0.79      0.22      0.35        67\n",
    "\n",
    "avg / total       0.78      0.76      0.73     22543\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SVM (k=rbf C=2**64 sigma=2**1): accuracy score = 0.765\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        dos       0.98      0.79      0.88      7458\n",
      "     normal       0.67      0.97      0.80      9710\n",
      "      probe       0.78      0.64      0.71      2421\n",
      "        r2l       0.63      0.11      0.18      2887\n",
      "        u2r       0.79      0.22      0.35        67\n",
      "\n",
      "avg / total       0.78      0.76      0.73     22543\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Run SVM on our Test set \n",
    "svc = SVC(C = C_selected, kernel = k_selected, gamma = gamma_selected)\n",
    "svc.fit(X_train_norm,y_train)\n",
    "svc_predictions = svc.predict(X_test_norm)\n",
    "score = svc.score(X_test_norm,y_test)\n",
    "print('\\nSVM (k=%s C=2**%d sigma=2**%d): accuracy score = %.3f' % \n",
    "              (k_selected,C_selected,gamma_selected,score))\n",
    "print(classification_report(y_test,svc_predictions, target_names = y_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCLUSION\n",
    "\n",
    "This SVM model is able to obtain ~76.5% accuracy against the NSL-KDD intrusion detection dataset.  It's precision is best with respect to DOS attacks (the highest volume attack in the dataset).  It's worst performance is on the R2L attacks (which are present in very few records in the dataset).  It's poor precision for NORMAL traffic also suggests much work would be needed to productionize such a system.  Other ML models should also be explored."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
